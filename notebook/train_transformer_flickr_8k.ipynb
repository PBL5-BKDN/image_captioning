{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T22:41:06.101348Z",
     "start_time": "2025-04-07T22:41:06.097275Z"
    }
   },
   "cell_type": "code",
   "source": [
    "EMBED_DIM = 300\n",
    "NUM_HEADS = 4\n",
    "MAX_SEQUENCE_LENGTH = 50\n",
    "BATCH_SIZE = 2\n",
    "VOCAB_SIZE = 8000\n",
    "UNITS = 512"
   ],
   "id": "b4515decab34d93",
   "outputs": [],
   "execution_count": 54
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-08T04:09:56.316474Z",
     "start_time": "2025-04-08T04:09:56.025137Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "from torchvision.models import ResNet50_Weights\n",
    "from torch import nn\n",
    "import torch\n",
    "from torchvision import models\n",
    "class CNNEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        # Sử dụng ResNet50 pretrained làm backbone\n",
    "        resnet = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "\n",
    "        self.backbone = nn.Sequential(*list(resnet.children())[:-2])  # Loại bỏ avgpool và fc\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.conv_proj = nn.Conv2d(2048, embed_dim, kernel_size=1)  # Dự phóng đặc trưng ResNet sang embed_dim\n",
    "    def forward(self, image):\n",
    "        \"\"\"\n",
    "        :param image: tensor of shape (B, 3, 224, 224)\n",
    "        :return: features tensor of shape (B, 49, embed_dim)\n",
    "        \"\"\"\n",
    "        features = self.backbone(image) #(B, 2048, 7, 7)\n",
    "        features = self.conv_proj(features) #(B, embed_dim , 7, 7)\n",
    "        features = features.flatten(2).transpose(1, 2)  # (B, 49, embed_dim)\n",
    "        return features\n",
    "\n",
    "fake_image = torch.rand(1,3,224,224)\n",
    "cnn_model = CNNEncoder(512)\n",
    "features = cnn_model(fake_image)\n",
    "print(features)\n"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.1023, -0.0380, -0.1043,  ...,  0.0120, -0.0228,  0.0770],\n",
      "         [-0.5755, -0.4306,  0.4012,  ...,  0.0873,  0.6671, -0.0862],\n",
      "         [ 0.5306,  0.1266, -0.0822,  ...,  0.3353,  0.6046,  0.0615],\n",
      "         ...,\n",
      "         [-0.0717,  0.0283, -0.2065,  ..., -0.1179,  0.1169, -0.2410],\n",
      "         [ 0.0912,  0.0845,  0.3961,  ..., -0.0771, -0.0927, -0.2596],\n",
      "         [-0.0027,  0.0200,  0.0037,  ..., -0.0194, -0.0277, -0.0739]]],\n",
      "       grad_fn=<TransposeBackward0>)\n"
     ]
    }
   ],
   "execution_count": 80
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T04:12:25.666095Z",
     "start_time": "2025-04-08T04:12:25.640730Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Định nghĩa Transformer Encoder\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "\n",
    "        self.layer_norm_1 = nn.LayerNorm(embed_dim)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.attention = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
    "        self.layer_norm_2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: (B, S, E)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        x = self.layer_norm_1(x)\n",
    "        x = self.fc(x)\n",
    "        attention_output, _ = self.attention(x, x, x)\n",
    "        output = self.layer_norm_2(x + attention_output)\n",
    "        return output\n",
    "\n",
    "model = TransformerEncoder(embed_dim=EMBED_DIM, num_heads=NUM_HEADS)\n",
    "dummy_input = torch.randn(BATCH_SIZE, MAX_SEQUENCE_LENGTH, EMBED_DIM)\n",
    "encoder_output = model(dummy_input)\n",
    "print(encoder_output)"
   ],
   "id": "c9f48e19324d7309",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 2.3458e-03, -1.1354e+00, -3.4171e-01,  ..., -1.0320e+00,\n",
      "          -4.3343e-01, -1.6673e-01],\n",
      "         [ 1.3954e+00, -1.2598e+00, -1.9444e-02,  ..., -1.1260e+00,\n",
      "           7.8159e-02,  8.3682e-01],\n",
      "         [-8.3563e-02, -7.0761e-01, -4.0396e-01,  ...,  1.0848e+00,\n",
      "           1.0095e+00, -2.4489e-01],\n",
      "         ...,\n",
      "         [-4.3441e-02, -1.2229e+00, -1.7695e-01,  ..., -4.8878e-02,\n",
      "           2.2392e-01, -2.1868e-01],\n",
      "         [ 2.2855e-01, -1.2376e+00, -4.7761e-01,  ..., -1.1419e+00,\n",
      "          -5.6921e-01,  5.7530e-01],\n",
      "         [ 1.4122e+00,  1.1692e+00,  2.0426e-01,  ...,  1.0484e+00,\n",
      "          -5.3385e-01,  1.1882e+00]],\n",
      "\n",
      "        [[-4.3900e-02,  5.3344e-01,  4.8977e-01,  ..., -1.0202e+00,\n",
      "           1.4918e+00,  5.8758e-01],\n",
      "         [ 2.4488e-02, -1.0517e+00,  1.4635e+00,  ..., -9.6917e-01,\n",
      "          -4.9653e-01, -2.6514e-01],\n",
      "         [ 1.8355e+00, -1.1179e+00,  8.0710e-01,  ..., -1.0090e+00,\n",
      "          -5.1617e-01,  1.9195e+00],\n",
      "         ...,\n",
      "         [ 3.7846e-02,  1.3091e-01,  6.3218e-01,  ..., -4.5527e-01,\n",
      "          -5.1604e-01,  2.4107e+00],\n",
      "         [ 9.1570e-02, -4.5723e-01, -4.8960e-01,  ..., -1.0148e+00,\n",
      "          -4.8634e-01,  1.4659e+00],\n",
      "         [ 3.0199e-02, -5.5315e-01,  1.9991e+00,  ..., -9.7525e-01,\n",
      "          -4.9407e-01, -2.6751e-01]]], grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "execution_count": 81
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T04:19:08.689041Z",
     "start_time": "2025-04-08T04:19:08.593576Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, embed_dim = 300, vocab_size = 10000, max_len_seq = 50):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_len_seq, embed_dim)\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "\n",
    "        :param x: tensor of shape (B, MAX_SEQUENCE_LENGTH)\n",
    "        :return: tensor of shape (B,MAX_SEQUENCE_LENGTH, EMBED_DIM)\n",
    "        \"\"\"\n",
    "        token_embedding = self.token_embedding(x) ## (B, S, E)\n",
    "        seq_length = x.shape[1]\n",
    "        positions = torch.arange(0, seq_length, device=x.device).unsqueeze(0)  # (1, S)\n",
    "        pos_embed = self.pos_embedding(positions)  # (1, S, E)\n",
    "        return token_embedding + pos_embed\n",
    "\n",
    "embed_model = Embedding(EMBED_DIM, VOCAB_SIZE, MAX_SEQUENCE_LENGTH)\n",
    "\n"
   ],
   "id": "31b1c772ad88d799",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[83]\u001B[39m\u001B[32m, line 19\u001B[39m\n\u001B[32m     16\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m token_embedding + pos_embed\n\u001B[32m     18\u001B[39m embed_model = Embedding(EMBED_DIM, VOCAB_SIZE, MAX_SEQUENCE_LENGTH)\n\u001B[32m---> \u001B[39m\u001B[32m19\u001B[39m output = \u001B[43membed_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfake_image\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     20\u001B[39m \u001B[38;5;28mprint\u001B[39m(output)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Desktop\\python_prj\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Desktop\\python_prj\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[83]\u001B[39m\u001B[32m, line 12\u001B[39m, in \u001B[36mEmbedding.forward\u001B[39m\u001B[34m(self, x)\u001B[39m\n\u001B[32m      6\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[32m      7\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m      8\u001B[39m \n\u001B[32m      9\u001B[39m \u001B[33;03m    :param x: tensor of shape (B, MAX_SEQUENCE_LENGTH)\u001B[39;00m\n\u001B[32m     10\u001B[39m \u001B[33;03m    :return: tensor of shape (B,MAX_SEQUENCE_LENGTH, EMBED_DIM)\u001B[39;00m\n\u001B[32m     11\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m12\u001B[39m     token_embedding = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtoken_embedding\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;66;03m## (B, S, E)\u001B[39;00m\n\u001B[32m     13\u001B[39m     seq_length = x.shape[\u001B[32m1\u001B[39m]\n\u001B[32m     14\u001B[39m     positions = torch.arange(\u001B[32m0\u001B[39m, seq_length, device=x.device).unsqueeze(\u001B[32m0\u001B[39m)  \u001B[38;5;66;03m# (1, S)\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Desktop\\python_prj\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Desktop\\python_prj\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Desktop\\python_prj\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\sparse.py:190\u001B[39m, in \u001B[36mEmbedding.forward\u001B[39m\u001B[34m(self, input)\u001B[39m\n\u001B[32m    189\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) -> Tensor:\n\u001B[32m--> \u001B[39m\u001B[32m190\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[43m.\u001B[49m\u001B[43membedding\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    191\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    192\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    193\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mpadding_idx\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    194\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmax_norm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    195\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mnorm_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    196\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mscale_grad_by_freq\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    197\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43msparse\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    198\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Desktop\\python_prj\\.venv\\Lib\\site-packages\\torch\\nn\\functional.py:2551\u001B[39m, in \u001B[36membedding\u001B[39m\u001B[34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001B[39m\n\u001B[32m   2545\u001B[39m     \u001B[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001B[39;00m\n\u001B[32m   2546\u001B[39m     \u001B[38;5;66;03m# XXX: equivalent to\u001B[39;00m\n\u001B[32m   2547\u001B[39m     \u001B[38;5;66;03m# with torch.no_grad():\u001B[39;00m\n\u001B[32m   2548\u001B[39m     \u001B[38;5;66;03m#   torch.embedding_renorm_\u001B[39;00m\n\u001B[32m   2549\u001B[39m     \u001B[38;5;66;03m# remove once script supports set_grad_enabled\u001B[39;00m\n\u001B[32m   2550\u001B[39m     _no_grad_embedding_renorm_(weight, \u001B[38;5;28minput\u001B[39m, max_norm, norm_type)\n\u001B[32m-> \u001B[39m\u001B[32m2551\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43membedding\u001B[49m\u001B[43m(\u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpadding_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscale_grad_by_freq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msparse\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mRuntimeError\u001B[39m: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)"
     ]
    }
   ],
   "execution_count": 83
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T08:57:47.898757Z",
     "start_time": "2025-04-08T08:57:47.738717Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self,units, embed_dim = 300, num_heads= 4, vocab_size=10000, max_len = 50):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "\n",
    "        self.embedding = Embedding(embed_dim, vocab_size=vocab_size, max_len_seq=max_len)\n",
    "\n",
    "        self.attention_1 = nn.MultiheadAttention(embed_dim, num_heads, dropout=0.1, batch_first=True)\n",
    "        self.layer_norm_1 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.attention_2 = nn.MultiheadAttention(embed_dim, num_heads, dropout=0.1,  batch_first=True)\n",
    "        self.layer_norm_2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "\n",
    "        self.ffn_layer_1 = nn.Sequential(\n",
    "            nn.Linear(embed_dim, units),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.dropout_1 = nn.Dropout(0.3)\n",
    "\n",
    "        self.ffn_layer_2 = nn.Sequential(\n",
    "            nn.Linear(units,embed_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.layer_norm_3 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.dropout_2 = nn.Dropout(0.5)\n",
    "\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(embed_dim, vocab_size),\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def get_causal_attention_mask(self, inputs):\n",
    "        batch_size, seq_len, _ = inputs.shape\n",
    "        mask = torch.tril(torch.ones((seq_len, seq_len), dtype=torch.int32, device=inputs.device)).bool()\n",
    "        mask = mask.unsqueeze(0).repeat(batch_size, 1, 1)\n",
    "        return mask\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, encoder_output, mask = None):\n",
    "        \"\"\"\n",
    "        :param input_ids: tensor of shape (B, MAX_SEQUENCE_LENGTH)\n",
    "        :param encoder_output:\n",
    "        :param mask:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        embeddings = self.embedding(input_ids) # (batch, seq_len, embed_dim)\n",
    "        print(embeddings.shape)\n",
    "        seq_len = embeddings.size(1)\n",
    "\n",
    "\n",
    "        # Tạo causal mask với kích thước (seq_len, seq_len)\n",
    "        causal_mask = torch.tril(torch.ones((seq_len, seq_len), device=embeddings.device)).bool()\n",
    "        key_padding_mask = None\n",
    "        if mask is not None:\n",
    "            # Giả sử mask có True ở vị trí hợp lệ, ta cần đảo lại (True cho padding)\n",
    "            key_padding_mask = ~mask.bool()\n",
    "\n",
    "\n",
    "        attn_output_1, _ = self.attention_1(\n",
    "            embeddings,\n",
    "            embeddings,\n",
    "            embeddings,\n",
    "            attn_mask=causal_mask,\n",
    "            key_padding_mask=key_padding_mask\n",
    "        )\n",
    "\n",
    "        out_1 = self.layer_norm_1(embeddings + attn_output_1)\n",
    "\n",
    "        attn_output_2, _ = self.attention_2(\n",
    "            query=out_1,\n",
    "            value=encoder_output,\n",
    "            key=encoder_output\n",
    "        )\n",
    "\n",
    "        out_2 = self.layer_norm_2(out_1 + attn_output_2)\n",
    "\n",
    "        ffn_output = self.ffn_layer_1(out_2)\n",
    "        ffn_output = self.dropout_1(ffn_output)\n",
    "        ffn_output = self.ffn_layer_2(ffn_output)\n",
    "\n",
    "        ffn_output = self.layer_norm_3(ffn_output + out_2)\n",
    "        ffn_output = self.dropout_2(ffn_output)\n",
    "        preds = self.out(ffn_output)\n",
    "        return preds\n",
    "\n",
    "\n",
    "decoder = TransformerDecoder(units=UNITS, embed_dim=EMBED_DIM, num_heads=NUM_HEADS, vocab_size=VOCAB_SIZE, max_len=MAX_SEQUENCE_LENGTH)\n",
    "input_ids = torch.randint(0, VOCAB_SIZE, (BATCH_SIZE, MAX_SEQUENCE_LENGTH))  # (B, S)\n",
    "encoder_output = torch.randn(BATCH_SIZE, MAX_SEQUENCE_LENGTH, EMBED_DIM)  # (B, S, E)\n",
    "\n",
    "output = decoder(input_ids, encoder_output)\n",
    "print(\"Output shape:\", output.shape)\n"
   ],
   "id": "dba22afe1fea213c",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[38;5;28;01mclass\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mTransformerDecoder\u001B[39;00m(\u001B[43mnn\u001B[49m.Module):\n\u001B[32m      2\u001B[39m     \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m,units, embed_dim = \u001B[32m300\u001B[39m, num_heads= \u001B[32m4\u001B[39m, vocab_size=\u001B[32m10000\u001B[39m, max_len = \u001B[32m50\u001B[39m):\n\u001B[32m      3\u001B[39m         \u001B[38;5;28msuper\u001B[39m(TransformerDecoder, \u001B[38;5;28mself\u001B[39m).\u001B[34m__init__\u001B[39m()\n",
      "\u001B[31mNameError\u001B[39m: name 'nn' is not defined"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T04:19:56.760722Z",
     "start_time": "2025-04-08T04:19:56.172350Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ImageCaptionModel(nn.Module):\n",
    "    def __init__(self, embed_dim=300, num_heads = 4, units = 512, vocab_size = 10000, max_len = 50):\n",
    "        super(ImageCaptionModel, self).__init__()\n",
    "        self.cnn_model = CNNEncoder(embed_dim)\n",
    "        self.encoder = TransformerEncoder(embed_dim, num_heads)\n",
    "        self.decoder = TransformerDecoder(units, embed_dim, num_heads, vocab_size, max_len)\n",
    "    def forward(self, images, inputs):\n",
    "        \"\"\"\n",
    "        :param images: tensor of shape (B, 3, 224, 224)\n",
    "        :param images:\n",
    "        :param input:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        features = self.cnn_model(images)\n",
    "        print(features)\n",
    "        encoded_features = self.encoder(features)\n",
    "        print(encoded_features)\n",
    "        mask = (inputs != 0)\n",
    "\n",
    "        outputs = self.decoder(inputs, encoded_features, mask=mask)\n",
    "        print(output)\n",
    "        return outputs\n",
    "\n",
    "# Tạo fake data\n",
    "def create_fake_data():\n",
    "    # Fake images: (B, 3, 224, 224) -> Giả lập đầu ra ResNet bằng (B, 2048, 7, 7)\n",
    "    fake_images = torch.randn(BATCH_SIZE, 3, 224, 224)  # Giả lập đầu ra sau ResNet\n",
    "\n",
    "    # Fake input_ids: (B, MAX_SEQUENCE_LENGTH)\n",
    "    # Giả sử 0 là padding token, các giá trị khác là token IDs ngẫu nhiên từ 1 đến VOCAB_SIZE-1\n",
    "    fake_input_ids = torch.randint(0, VOCAB_SIZE, (BATCH_SIZE, MAX_SEQUENCE_LENGTH))\n",
    "    fake_input_ids[0, 40:] = 0  # Thêm padding cho batch 0\n",
    "    fake_input_ids[1, 45:] = 0  # Thêm padding cho batch 1\n",
    "\n",
    "    return fake_images, fake_input_ids\n",
    "\n",
    "model = ImageCaptionModel(\n",
    "        embed_dim=EMBED_DIM,\n",
    "        num_heads=NUM_HEADS,\n",
    "        units=UNITS,\n",
    "        vocab_size=VOCAB_SIZE,\n",
    "        max_len=MAX_SEQUENCE_LENGTH\n",
    "    )\n",
    "\n",
    "# Tạo dữ liệu giả\n",
    "fake_images, fake_input_ids = create_fake_data()\n",
    "\n",
    "# Chạy mô hình\n",
    "model.eval()  # Chuyển sang chế độ đánh giá để tắt dropout\n",
    "with torch.no_grad():\n",
    "    outputs = model(fake_images, fake_input_ids)\n",
    "\n",
    "# Kiểm tra kích thước đầu ra\n",
    "print(\"Input images shape:\", fake_images.shape)\n",
    "print(\"Input IDs shape:\", fake_input_ids.shape)\n",
    "print(\"Output shape:\", outputs.shape)\n",
    "print(\"Test passed: Output shape is correct!\")\n"
   ],
   "id": "f8c382687f08a918",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.4842, -0.2637,  0.0942,  ...,  0.3385,  0.5352,  0.0480],\n",
      "         [ 0.0073,  0.0668,  0.1749,  ...,  0.0199, -0.0232,  0.0013],\n",
      "         [ 0.0352,  0.0659,  0.1235,  ..., -0.2071, -0.1315, -0.0244],\n",
      "         ...,\n",
      "         [ 0.4003,  0.1956,  0.3757,  ..., -0.2038, -0.1387,  0.1403],\n",
      "         [ 0.4169,  0.2573,  0.3274,  ..., -0.2580, -0.0329,  0.2286],\n",
      "         [ 0.3901,  0.2516,  0.3742,  ..., -0.4066, -0.3367,  0.2889]],\n",
      "\n",
      "        [[ 0.5987, -0.4070,  0.0554,  ...,  0.5139,  0.7587,  0.0194],\n",
      "         [-0.1266,  0.0816,  0.2008,  ...,  0.0689, -0.0855, -0.0482],\n",
      "         [ 0.1053,  0.0011,  0.2195,  ..., -0.1870, -0.1799,  0.0600],\n",
      "         ...,\n",
      "         [ 0.1246,  0.1640,  0.1929,  ..., -0.3208, -0.3308,  0.2767],\n",
      "         [ 0.4593,  0.2201,  0.4284,  ..., -0.2924, -0.3859,  0.3219],\n",
      "         [ 0.3205,  0.2608,  0.3638,  ..., -0.2398, -0.2610,  0.1867]]])\n",
      "tensor([[[-1.0807, -1.1966, -0.4577,  ..., -0.6663,  1.2143,  1.7226],\n",
      "         [-0.4045, -1.1544,  1.6457,  ..., -0.6727, -0.5362, -0.3499],\n",
      "         [-1.0369, -0.4277, -0.4847,  ..., -0.6741, -0.5168, -0.3434],\n",
      "         ...,\n",
      "         [-0.8794,  0.2985, -0.1026,  ..., -0.7191, -0.5782, -0.4047],\n",
      "         [-1.0648,  0.3870,  0.3644,  ..., -0.7225, -0.5811, -0.4084],\n",
      "         [-0.6121,  0.6822, -0.5374,  ..., -0.7754, -0.6280, -0.4563]],\n",
      "\n",
      "        [[-1.0825, -1.1212, -0.4637,  ..., -0.6157,  0.5902,  1.5156],\n",
      "         [-0.3222, -1.1246,  1.7920,  ..., -0.6733, -0.6078, -0.3815],\n",
      "         [-0.0252, -1.0656, -0.5190,  ..., -0.6752, -0.6020, -0.3832],\n",
      "         ...,\n",
      "         [-1.0857,  1.5000, -0.5322,  ..., -0.6873, -0.6111, -0.4007],\n",
      "         [-0.8141,  1.1123, -0.2768,  ..., -0.7292, -0.6547, -0.4431],\n",
      "         [-0.4229,  1.3560, -0.6193,  ..., -0.7773, -0.7000, -0.4849]]])\n",
      "torch.Size([2, 50, 300])\n",
      "tensor([[[ 0.7455,  0.2803,  0.1126,  ..., -0.3906,  0.5739,  0.3775],\n",
      "         [ 0.1361,  0.3297,  0.0321,  ..., -1.2274,  0.9740, -0.6176],\n",
      "         [-0.5216, -1.4934, -0.7013,  ..., -1.3818,  1.0662, -0.4549],\n",
      "         ...,\n",
      "         [ 0.5602, -0.0823,  1.2631,  ..., -1.3008,  0.8801, -0.2016],\n",
      "         [-1.3323,  1.2862,  0.1733,  ..., -0.6884, -0.4098,  0.8976],\n",
      "         [    nan,     nan,     nan,  ...,     nan,     nan,     nan]],\n",
      "\n",
      "        [[ 0.8196,  0.0183, -0.8079,  ...,  0.2945,  0.9182,  1.0525],\n",
      "         [-1.2194,  0.4678, -0.2736,  ..., -0.8002,  0.8630, -0.2370],\n",
      "         [-0.4971, -0.3068, -0.2031,  ...,  0.7880, -0.5929, -0.9484],\n",
      "         ...,\n",
      "         [ 1.1473,  0.0192,  0.4855,  ..., -0.9081, -1.2523,  0.8462],\n",
      "         [-0.5339, -0.0050,  1.0059,  ...,  0.4624,  0.2030,  0.2749],\n",
      "         [    nan,     nan,     nan,  ...,     nan,     nan,     nan]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Input images shape: torch.Size([2, 3, 224, 224])\n",
      "Input IDs shape: torch.Size([2, 50])\n",
      "Output shape: torch.Size([2, 50, 8000])\n",
      "Test passed: Output shape is correct!\n"
     ]
    }
   ],
   "execution_count": 84
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T14:54:45.398251Z",
     "start_time": "2025-04-07T14:54:45.320884Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "fd24c5f359a9c6ab",
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\NguyenPC\\\\Desktop\\\\dataset/Images\\\\1000268201_693b08cb0e.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mFileNotFoundError\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[49]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[43mtrain_dataset\u001B[49m\u001B[43m[\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m]\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Desktop\\python_prj\\data\\CustomDataset.py:41\u001B[39m, in \u001B[36mCustomDataset.__getitem__\u001B[39m\u001B[34m(self, index)\u001B[39m\n\u001B[32m     39\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__getitem__\u001B[39m(\u001B[38;5;28mself\u001B[39m, index):\n\u001B[32m     40\u001B[39m     img_path = os.path.join(images_dir, \u001B[38;5;28mself\u001B[39m.data[index])\n\u001B[32m---> \u001B[39m\u001B[32m41\u001B[39m     image = \u001B[38;5;28mself\u001B[39m.transform(\u001B[43mImage\u001B[49m\u001B[43m.\u001B[49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg_path\u001B[49m\u001B[43m)\u001B[49m.convert(\u001B[33m'\u001B[39m\u001B[33mRGB\u001B[39m\u001B[33m'\u001B[39m))\n\u001B[32m     43\u001B[39m     label = text_processing.caption_preprocessing(\u001B[38;5;28mself\u001B[39m.labels[index])\n\u001B[32m     44\u001B[39m     caption_tokens = [\u001B[38;5;28mself\u001B[39m.word2idx.get(token, \u001B[38;5;28mself\u001B[39m.word2idx[\u001B[33m\"\u001B[39m\u001B[33m<UNK>\u001B[39m\u001B[33m\"\u001B[39m]) \u001B[38;5;28;01mfor\u001B[39;00m token \u001B[38;5;129;01min\u001B[39;00m label.split()]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Desktop\\python_prj\\.venv\\Lib\\site-packages\\PIL\\Image.py:3465\u001B[39m, in \u001B[36mopen\u001B[39m\u001B[34m(fp, mode, formats)\u001B[39m\n\u001B[32m   3462\u001B[39m     filename = os.fspath(fp)\n\u001B[32m   3464\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m filename:\n\u001B[32m-> \u001B[39m\u001B[32m3465\u001B[39m     fp = \u001B[43mbuiltins\u001B[49m\u001B[43m.\u001B[49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mrb\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m   3466\u001B[39m     exclusive_fp = \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m   3467\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[31mFileNotFoundError\u001B[39m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\NguyenPC\\\\Desktop\\\\dataset/Images\\\\1000268201_693b08cb0e.jpg'"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=w2i[\"<PAD>\"])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr= learning_rate)\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "def train():\n",
    "    best_val_loss = float(\"inf\")\n",
    "    stopping_counter = 0\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        train_loader = tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{epochs} [Training]\")\n",
    "        for images, inputs, targets in train_loader:\n",
    "            images, inputs, targets = images.to(device), inputs.to(device), targets.to(device)\n",
    "            print(images)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(images, inputs) #(B, MAX_LEN - 1, VOCAB_SIZE)\n",
    "\n",
    "            loss = criterion(output.reshape(-1, VOCAB_SIZE), targets.reshape(-1))\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        total_test_loss = 0\n",
    "        test_loader = tqdm(test_dataloader, desc=f\"Epoch {epoch + 1}/{epochs} [Validation]\")\n",
    "        with torch.no_grad():\n",
    "            for images, inputs, targets in test_loader:\n",
    "                images, inputs, targets = images.to(device), inputs.to(device), targets.to(device)\n",
    "                output = model(images, inputs)\n",
    "                loss = criterion(output.reshape(-1, VOCAB_SIZE), targets.reshape(-1))\n",
    "                total_test_loss += loss.item()\n",
    "\n",
    "        avg_test_loss = total_test_loss / len(test_dataloader)\n",
    "        val_losses.append(avg_test_loss)\n",
    "        print(f\"\\nEpoch {epoch + 1}/{epochs} | Train Loss: {avg_train_loss:.4f} | Test Loss: {avg_test_loss:.4f}\")\n",
    "\n",
    "        if avg_test_loss < best_val_loss - min_delta:\n",
    "            best_val_loss = avg_test_loss\n",
    "            stopping_counter = 0\n",
    "            torch.save(model.state_dict(), \"best_model_flick.pth\")  # Lưu model tốt nhất\n",
    "        else:\n",
    "            stopping_counter += 1\n",
    "            if stopping_counter >= patience:\n",
    "                print(\"Early stopping triggered!\")\n",
    "                break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Vẽ biểu đồ\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label=\"Train Loss\", marker=\"o\")\n",
    "    plt.plot(val_losses, label=\"Validation Loss\", marker=\"o\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Training & Validation Loss\")\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    plt.savefig(\"train_loss.png\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ],
   "id": "869e83749c89ca47"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
